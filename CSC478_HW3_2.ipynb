{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Becky Jacob\n",
    "#CSC478 Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "%matplotlib inline\n",
    "import os\n",
    "from sklearn import feature_selection\n",
    "from sklearn import cross_validation\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "#QUESTION 2\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create your own distance function that uses Cosine similarity. \\nThis is the distance function you will use to pass to the kMeans function.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################\n",
    "#Part a\n",
    "#################\n",
    "'''Create your own distance function that uses Cosine similarity. \n",
    "This is the distance function you will use to pass to the kMeans function.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosSim(vecA, vecB):\n",
    "    sim = (np.dot(vecA,vecB))/((sqrt(np.dot(vecA,vecA))*(sqrt(np.dot(vecB,vecB)))))\n",
    "    return 1-sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Load the data set. Matrix provided has terms as rows and documents as columns. \\nTake the transpose of this matrix so that your main matrix is doc x term. \\nSplit the data set and set aside 20% for later use (see below). \\nUse the 80% segment for clustering in the next part. \\nThe 20% portion must be a random subset.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################\n",
    "#Part b\n",
    "#################\n",
    "'''Load the data set. Matrix provided has terms as rows and documents as columns. \n",
    "Take the transpose of this matrix so that your main matrix is doc x term. \n",
    "Split the data set and set aside 20% for later use (see below). \n",
    "Use the 80% segment for clustering in the next part. \n",
    "The 20% portion must be a random subset.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TxD = np.genfromtxt('matrix.txt', delimiter=',')\n",
    "classes = np.genfromtxt('classes.txt', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9328, 2500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TxD.shape\n",
    "#classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.],\n",
       "       [  1.,   1.],\n",
       "       [  2.,   1.],\n",
       "       [  3.,   1.],\n",
       "       [  4.,   2.],\n",
       "       [  5.,   1.],\n",
       "       [  6.,   3.],\n",
       "       [  7.,   4.],\n",
       "       [  8.,   1.],\n",
       "       [  9.,   1.],\n",
       "       [ 10.,   2.],\n",
       "       [ 11.,   4.],\n",
       "       [ 12.,   1.],\n",
       "       [ 13.,   0.],\n",
       "       [ 14.,   0.],\n",
       "       [ 15.,   2.],\n",
       "       [ 16.,   0.],\n",
       "       [ 17.,   0.],\n",
       "       [ 18.,   3.],\n",
       "       [ 19.,   4.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cla = classes[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1., ...,  3.,  4.,  2.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DxT = TxD.T\n",
    "#ClassT = classes.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 9328)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DxT.shape\n",
    "#ClassT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 9328)\n",
      "(2000, 9328)\n",
      "(500,)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(DxT, cla, test_size=0.2, random_state=35)\n",
    "\n",
    "np.set_printoptions(precision=4, linewidth=80, suppress=True)\n",
    "print (x_test.shape)\n",
    "print (x_train.shape)\n",
    "print (y_test.shape)\n",
    "print (y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transform the term-frequencies to tfxidf values. \\nMaintain DF values for each of the terms in the dictionary. \\n[Note: if you run into problems due to limited computational resources, \\nyou may prune the data by removing all terms with low DF values, e.g., \\nterms that appear in less than 10 documents. \\nBe sure to maintain the correspondence between the dictionary terms and \\nthe matrix rows.]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################\n",
    "#Part c\n",
    "#################\n",
    "'''Transform the term-frequencies to tfxidf values. \n",
    "Maintain DF values for each of the terms in the dictionary. \n",
    "[Note: if you run into problems due to limited computational resources, \n",
    "you may prune the data by removing all terms with low DF values, e.g., \n",
    "terms that appear in less than 10 documents. \n",
    "Be sure to maintain the correspondence between the dictionary terms and \n",
    "the matrix rows.]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  5 16 ...,  3  3  3]]\n",
      "[[ 8.38  8.64  6.97 ...,  9.38  9.38  9.38]\n",
      " [ 8.38  8.64  6.97 ...,  9.38  9.38  9.38]\n",
      " [ 8.38  8.64  6.97 ...,  9.38  9.38  9.38]\n",
      " ..., \n",
      " [ 8.38  8.64  6.97 ...,  9.38  9.38  9.38]\n",
      " [ 8.38  8.64  6.97 ...,  9.38  9.38  9.38]\n",
      " [ 8.38  8.64  6.97 ...,  9.38  9.38  9.38]]\n"
     ]
    }
   ],
   "source": [
    "#TFxIDF weights\n",
    "numTerms = x_train.shape[1]\n",
    "numDocs = x_train.shape[0]\n",
    "NMatrix = np.ones(np.shape(x_train), dtype=float)*numDocs\n",
    "DF = np.array([(x_train!=0).sum(0)])\n",
    "#DivM = np.divide(NMatrix, DF)\n",
    "#IDF = np.log2(DivM)\n",
    "IDF = np.log2(np.divide(NMatrix, DF))\n",
    "pos_inf = float('+inf')\n",
    "IDF[IDF == pos_inf] = 0.0\n",
    "np.set_printoptions(precision=2)\n",
    "print(DF)\n",
    "print (IDF)\n",
    "DT_tfidf = x_train*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 9328)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove terms with DF<10 - I ended up not using this, because it wasn't taking too long to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "newM = np.vstack([DT_tfidf, DF])\n",
    "newM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduced = newM[:,newM[2000]>10]\n",
    "reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "final = reduced[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Perform Kmeans clustering on the training data.\\nWrite a function to display the top N terms in each cluster along with \\nthe cluster DF values for each term and the size of the cluster. \\nCluster DF value for a term t in a cluster C is the percentage of docs \\nin cluster C in which term t appears. Sort the terms in decreasing order \\nof the DF percentage. Here is an example of how this output might look \\nlike (here the top 10 terms for 3 of the 5 clusters are displayed in \\ndecreasing order of cluster DF values, but the mean frequnecy from the \\ncluster centroid is also shown). [Extra Credit: use your favorite third \\nparty tool, ideally with a Python based API, to create a word cloud for \\neach cluster based on the in-cluster DF values.]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################\n",
    "#Part d\n",
    "#################\n",
    "'''Perform Kmeans clustering on the training data.\n",
    "Write a function to display the top N terms in each cluster along with \n",
    "the cluster DF values for each term and the size of the cluster. \n",
    "Cluster DF value for a term t in a cluster C is the percentage of docs \n",
    "in cluster C in which term t appears. Sort the terms in decreasing order \n",
    "of the DF percentage. Here is an example of how this output might look \n",
    "like (here the top 10 terms for 3 of the 5 clusters are displayed in \n",
    "decreasing order of cluster DF values, but the mean frequnecy from the \n",
    "cluster centroid is also shown). [Extra Credit: use your favorite third \n",
    "party tool, ideally with a Python based API, to create a word cloud for \n",
    "each cluster based on the in-cluster DF values.]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randCent(dataSet, k):\n",
    "        n = dataSet.shape[1]\n",
    "        centroids = np.zeros((k,n), dtype=float)\n",
    "        for j in range(n): #create random cluster centers\n",
    "                minJ = min(dataSet[:,j])\n",
    "                rangeJ = float(max(dataSet[:,j]) - minJ)\n",
    "                centroids[:,j] = minJ + rangeJ * random.rand(k)\n",
    "        return centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kMeans(dataSet, k, distMeas=cosSim, createCent=randCent):\n",
    "    m = dataSet.shape[0]\n",
    "    #print(m)\n",
    "    clusterAssment = np.zeros((m,2))#create mat to assign data points \n",
    "                                      #to a centroid, also holds SE of each point\n",
    "    #print (clusterAssment.shape)\n",
    "    centroids = createCent(dataSet, k)\n",
    "    #print (centroids)\n",
    "    clusterChanged = True\n",
    "    while clusterChanged:\n",
    "        clusterChanged = False\n",
    "        for i in range(m):#for each data point assign it to the closest centroid\n",
    "            minDist = np.inf; minIndex = -1\n",
    "            for j in range(k):\n",
    "                distJI = distMeas(centroids[j,:],dataSet[i,:])\n",
    "                if distJI < minDist:\n",
    "                    minDist = distJI; minIndex = j\n",
    "            if clusterAssment[i,0] != minIndex: clusterChanged = True\n",
    "            clusterAssment[i,:] = minIndex,minDist**2\n",
    "        #print (centroids)\n",
    "        for cent in range(k):#recalculate centroids\n",
    "            ptsInClust = dataSet[np.nonzero(clusterAssment[:,0]==cent)[0]] #get all the point in this cluster - Note: this was incorrect in the original distribution.\n",
    "            if(len(ptsInClust)!=0):\n",
    "                centroids[cent,:] = np.mean(ptsInClust, axis=0)\n",
    "    return centroids, clusterAssment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_tfidf, clusters_tfidf = kMeans(DT_tfidf, 3, cosSim, randCent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = np.genfromtxt('terms.txt',dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06,  0.  ,  0.32, ...,  0.06,  0.04,  0.08],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  1.04,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.1 ,  0.02, ...,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.77],\n",
       "       [ 0.  ,  0.84],\n",
       "       [ 2.  ,  0.85],\n",
       "       ..., \n",
       "       [ 2.  ,  0.49],\n",
       "       [ 0.  ,  0.86],\n",
       "       [ 2.  ,  0.66]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TopNTerms(N, k, data, terms):\n",
    "    centroids_tfidf, clusters_tfidf = kMeans(data, k, cosSim, randCent)\n",
    "    clus = clusters_tfidf[:,0]\n",
    "    pdDF = pd.DataFrame(data)\n",
    "    pdDF.columns = [terms[i] for i in range(len(terms))]\n",
    "    #print (pdDF)\n",
    "    pdDF['predClass'] = pd.Series(clus)\n",
    "    for i in range(0,k):\n",
    "        clusData = pdDF.loc[pdDF['predClass']==i]\n",
    "        NumDocs = clusData.astype(bool).sum(axis=0).tolist()\n",
    "        temp = [(x/len(clusData))*100 for x in NumDocs]\n",
    "        PerDoc = [ '%.4f' % d for d in temp]\n",
    "        lstTemp = pd.DataFrame(list(zip(NumDocs,PerDoc)))\n",
    "        lstTemp.columns = ['Frequency', 'Percentage']\n",
    "        lstTemp.insert(loc=0,column='Words',value=pd.Series(terms))\n",
    "        lstFinal = lstTemp.sort_values(['Percentage'],ascending=False)\n",
    "        print ('Cluster: {0}  results'.format(i))\n",
    "        print ('Number of Documents in Cluster: {0}'.format(len(clusData)))\n",
    "        print (lstFinal.head(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0  results\n",
      "Number of Documents in Cluster: 1986\n",
      "       Words  Frequency Percentage\n",
      "5975    part        197     9.9194\n",
      "3419   great        197     9.9194\n",
      "2174  differ        196     9.8691\n",
      "3239   gener        196     9.8691\n",
      "732   better        195     9.8187\n",
      "Cluster: 1  results\n",
      "Number of Documents in Cluster: 1\n",
      "        Words  Frequency Percentage\n",
      "9328      NaN          1   100.0000\n",
      "5758   offset          1   100.0000\n",
      "8470      two          1   100.0000\n",
      "878   borland          1   100.0000\n",
      "895     bounc          1   100.0000\n",
      "Cluster: 2  results\n",
      "Number of Documents in Cluster: 7\n",
      "     Words  Frequency Percentage\n",
      "5255  mnui          6    85.7143\n",
      "798    biz          6    85.7143\n",
      "4835    ma          6    85.7143\n",
      "9307   zkn          6    85.7143\n",
      "9306   zkm          6    85.7143\n",
      "Cluster: 3  results\n",
      "Number of Documents in Cluster: 6\n",
      "     Words  Frequency Percentage\n",
      "5826    oq          5    83.3333\n",
      "4725    lk          5    83.3333\n",
      "3482    gz          5    83.3333\n",
      "7664    sp          5    83.3333\n",
      "6295    pp          5    83.3333\n",
      "Cluster: 4  results\n",
      "Number of Documents in Cluster: 0\n",
      "         Words  Frequency Percentage\n",
      "0           aa        0.0        nan\n",
      "6196  platform        0.0        nan\n",
      "6216      plug        0.0        nan\n",
      "6217   plugger        0.0        nan\n",
      "6218    plugin        0.0        nan\n"
     ]
    }
   ],
   "source": [
    "TopNTerms(5,5,x_train, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using the cluster assignments from Kmeans clustering, \\ncompare your 5 clusters to the 5 pre-assigned classes by computing \\nthe Completeness and Homogeneity values.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################\n",
    "#Part e\n",
    "#################\n",
    "'''Using the cluster assignments from Kmeans clustering, \n",
    "compare your 5 clusters to the 5 pre-assigned classes by computing \n",
    "the Completeness and Homogeneity values.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 ..., 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "newC = np.ravel(clusters_tfidf.T[0])\n",
    "newC = newC.astype(int)\n",
    "print (newC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import completeness_score, homogeneity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815860793579\n"
     ]
    }
   ],
   "source": [
    "print (completeness_score(y_train,newC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.275284108844\n"
     ]
    }
   ],
   "source": [
    "print (homogeneity_score(y_train,newC))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
